{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "SUKUXK9h8XgL",
      "metadata": {
        "id": "SUKUXK9h8XgL"
      },
      "source": [
        "#**LangChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qo1DQXM18bgL",
      "metadata": {
        "id": "qo1DQXM18bgL"
      },
      "source": [
        "LangChain is a framework for developing applications powered by language models.\n",
        "\n",
        "- GitHub: https://github.com/hwchase17/langchain\n",
        "- Docs: https://python.langchain.com/v0.2/docs/introduction/\n",
        "\n",
        "### Overview:\n",
        "- Installation\n",
        "- LLMs\n",
        "- Prompt Templates\n",
        "- Chains\n",
        "- Agents and Tools\n",
        "- Memory\n",
        "- Document Loaders\n",
        "- Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09CgA1RZkiC4",
      "metadata": {
        "id": "09CgA1RZkiC4"
      },
      "source": [
        "#**01: Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X4tDdLTjkkk_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4tDdLTjkkk_",
        "outputId": "ad6a4acc-980f-4f4a-80ce-68720f04a9e0"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sQHZiF38-Cps",
      "metadata": {
        "id": "sQHZiF38-Cps"
      },
      "source": [
        "#**02: Setup the Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9-mFf0Ql-KX2",
      "metadata": {
        "id": "9-mFf0Ql-KX2"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed0dc6a",
      "metadata": {
        "id": "9ed0dc6a"
      },
      "source": [
        "##**03: Large Language Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516GZwvpnVpV",
      "metadata": {
        "id": "516GZwvpnVpV"
      },
      "source": [
        "The basic building block of LangChain is a Large Language Model which takes text as input and generates more text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FDyNMY3sRMc",
      "metadata": {
        "id": "4FDyNMY3sRMc"
      },
      "source": [
        "Suppose we want to generate a company name based on the company description, so we will first initialize an OpenAI wrapper. In this case, since we want the output to be more random, we will intialize our model with high temprature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eLqFwlXaH8f4",
      "metadata": {
        "id": "eLqFwlXaH8f4"
      },
      "source": [
        "The temperature parameter adjusts the randomness of the output. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rMOonq5OH97v",
      "metadata": {
        "id": "rMOonq5OH97v"
      },
      "source": [
        "temperature value--> how creative we want our model to be\n",
        "\n",
        "0 ---> temperature it means model is  very safe it is not taking any bets.\n",
        "\n",
        "1 --> it will take risk it might generate wrong output but it is very creative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M9Y34zmZ8xyc",
      "metadata": {
        "id": "M9Y34zmZ8xyc"
      },
      "source": [
        "A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TB5tAUbT92Z7",
      "metadata": {
        "id": "TB5tAUbT92Z7"
      },
      "source": [
        "#**Open AI**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BszO_ZXrs95T",
      "metadata": {
        "id": "BszO_ZXrs95T"
      },
      "source": [
        "#**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w-az-0Ex9CaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-az-0Ex9CaD",
        "outputId": "5e56a21b-2286-4c24-9af2-b34cecd18017"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lJEy652utDdM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJEy652utDdM",
        "outputId": "7b22584d-d6ff-4960-bcee-2a978fe8a33c"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n_nF4R5EtN_k",
      "metadata": {
        "id": "n_nF4R5EtN_k"
      },
      "source": [
        "And now we will pass in text and get  predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VIUqmBl3tUgj",
      "metadata": {
        "id": "VIUqmBl3tUgj"
      },
      "outputs": [],
      "source": [
        "text=\"What would be a good company name for a company that makes colorful socks?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7itCa0q9rn7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7itCa0q9rn7",
        "outputId": "13cf4864-4d03-486e-ec8a-d204c4d1220b"
      },
      "outputs": [],
      "source": [
        "print(llm.predict(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2KE0Fngs9daM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KE0Fngs9daM",
        "outputId": "e8ef4852-f282-4760-f1e0-356f280ee163"
      },
      "outputs": [],
      "source": [
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-s5lupvjFLVz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5lupvjFLVz",
        "outputId": "92c1cfef-c8dd-424d-84f7-98f7a3ac83c2"
      },
      "outputs": [],
      "source": [
        "print(llm.invoke(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EJIQT1FSn0Gl",
      "metadata": {
        "id": "EJIQT1FSn0Gl"
      },
      "source": [
        "#**Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa352d5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa352d5f",
        "outputId": "355c64b7-0400-4b1a-fe79-f52193a4b12d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "name = llm.predict(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56e8581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b56e8581",
        "outputId": "bdfae42c-1d40-4b9c-eb1a-718de7b5757c"
      },
      "outputs": [],
      "source": [
        "response=llm(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bj6wjnKZ-bgU",
      "metadata": {
        "id": "bj6wjnKZ-bgU"
      },
      "source": [
        "#**Hugging Face**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iTsUW116-th1",
      "metadata": {
        "id": "iTsUW116-th1"
      },
      "source": [
        "#**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDMLw7Yr-nQK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMLw7Yr-nQK",
        "outputId": "287688c8-816d-4c9f-f2c2-14e138537538"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B4w0ultA-icd",
      "metadata": {
        "id": "B4w0ultA-icd"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W-pl8cXk-ie7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "W-pl8cXk-ie7",
        "outputId": "7b8a4854-2e4d-4a7f-8ce6-1b94d3b6a994"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/google/flan-t5-xl\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "llm(\"translate English to German: How old are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2MOh4uIm-xDQ",
      "metadata": {
        "id": "2MOh4uIm-xDQ"
      },
      "source": [
        "#**Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dmAwr5-d-z6F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmAwr5-d-z6F",
        "outputId": "f86e804c-ee6c-4868-f0ca-97f301304d6f"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "# name = llm.predict(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n",
        "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0782a2dd",
      "metadata": {
        "id": "0782a2dd"
      },
      "source": [
        "##**04: Prompt Templates**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jszTHb6J_dNV",
      "metadata": {
        "id": "jszTHb6J_dNV"
      },
      "source": [
        "Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case\n",
        "\n",
        "LangChain faciliates prompt management and optimization.\n",
        "\n",
        "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unU1DcEv7TWh",
      "metadata": {
        "id": "unU1DcEv7TWh"
      },
      "source": [
        "In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IWqka6F_93QB",
      "metadata": {
        "id": "IWqka6F_93QB"
      },
      "source": [
        "#**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a306b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a306b9d",
        "outputId": "7c72aac5-20ad-4113-84ae-cb3a8d8c7d99",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(cuisine=\"indian\")\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qlKeWd7B95-R",
      "metadata": {
        "id": "qlKeWd7B95-R"
      },
      "source": [
        "#**Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qqJZBS9u8534",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qqJZBS9u8534",
        "outputId": "82018ddc-2af8-4a7d-f3e6-3d5ab9579bc9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O3YEt1YZKJHr",
      "metadata": {
        "id": "O3YEt1YZKJHr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "af406b92",
      "metadata": {
        "id": "af406b92"
      },
      "source": [
        "##**05: Chains**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vGaSSUAIBHdU",
      "metadata": {
        "id": "vGaSSUAIBHdU"
      },
      "source": [
        "Combine LLMs and Prompts in multi-step workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lcjlXP7z_-k6",
      "metadata": {
        "id": "lcjlXP7z_-k6"
      },
      "source": [
        "Now as we have the  **model**:\n",
        "\n",
        "\n",
        "  llm = OpenAI(temperature=0.9)\n",
        "\n",
        "\n",
        "and the **Prompt Template**:\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "\n",
        "\n",
        "prompt.format(product=\"colorful socks\")\n",
        "\n",
        "\n",
        "Now using Chains we will link together model and the PromptTemplate and other Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mIJx5zL2BbHJ",
      "metadata": {
        "id": "mIJx5zL2BbHJ"
      },
      "source": [
        "The simplest and most common type of Chain is LLMChain, which passes the input first to Prompt Template and then to Large Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5icZHtlDFrpI",
      "metadata": {
        "id": "5icZHtlDFrpI"
      },
      "source": [
        "LLMChain is responsible to execute the PromptTemplate, For every PromptTemplate we will specifically have an LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MAUSugfLCZH-",
      "metadata": {
        "id": "MAUSugfLCZH-"
      },
      "source": [
        "#**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22NEqcvGGvHJ",
      "metadata": {
        "id": "22NEqcvGGvHJ"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bK-KESsGGOhY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bK-KESsGGOhY",
        "outputId": "2301fc7b-8508-401f-8196-66978d6bc0f0"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8KjGw4iXGUGJ",
      "metadata": {
        "id": "8KjGw4iXGUGJ"
      },
      "source": [
        "Whatever input text i am giving that will get assigned to this particular variable that is **product**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1gatUl_ICZOP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gatUl_ICZOP",
        "outputId": "744d4ffb-e8b3-47f8-cd70-acdc9b0756dc"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "response= chain.run(\"colorful socks\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O93s1iRICXNv",
      "metadata": {
        "id": "O93s1iRICXNv"
      },
      "source": [
        "#**Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qV_H_EGCG-OR",
      "metadata": {
        "id": "qV_H_EGCG-OR"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uLtIkYe6G7xK",
      "metadata": {
        "id": "uLtIkYe6G7xK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba65c213",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba65c213",
        "outputId": "d004f5b6-5733-44b2-bb5a-3229941a99ab",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "response=chain.run(\"Mexican\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ccee75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ccee75",
        "outputId": "47d15f2d-aecf-4056-e43a-8b1e24cfd19b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
        "response=chain.run(\"Mexican\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EMd9OQVNH7lK",
      "metadata": {
        "id": "EMd9OQVNH7lK"
      },
      "source": [
        "**Can we combine Multiple PromptTemplates, We will try to combine Multiple PromptTemplates**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nv_tlKtLJLIZ",
      "metadata": {
        "id": "nv_tlKtLJLIZ"
      },
      "source": [
        "**The output from the first PromptTemplate is passed to the next PromptTemplate as input**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-6_6H-BJl9L",
      "metadata": {
        "id": "a-6_6H-BJl9L"
      },
      "source": [
        "#**To combine the Chain and  to set a sequence for that we use SimpleSequentialChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a98d9f",
      "metadata": {
        "id": "87a98d9f"
      },
      "source": [
        "##**Simple Sequential Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21098937",
      "metadata": {
        "id": "21098937"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fd9a79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9fd9a79",
        "outputId": "fa05334c-f200-49b9-d246-3d4f73510214",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
        "\n",
        "content = chain.run(\"indian\")\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "njqmmiouJ6Uc",
      "metadata": {
        "id": "njqmmiouJ6Uc"
      },
      "source": [
        "**There is a issue with SimpleSequentialChain it only shows last input information**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hKVVpZo8KC38",
      "metadata": {
        "id": "hKVVpZo8KC38"
      },
      "source": [
        "#**To show the entire information i will use SequentialChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0386d05c",
      "metadata": {
        "id": "0386d05c"
      },
      "source": [
        "##**Sequential Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49dc0fae",
      "metadata": {
        "id": "49dc0fae"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dea8402",
      "metadata": {
        "id": "9dea8402"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec1be10",
      "metadata": {
        "id": "1ec1be10"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [name_chain, food_items_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name', \"menu_items\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4653c540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4653c540",
        "outputId": "7d69312f-1a54-4f94-a92c-9629a2423c03"
      },
      "outputs": [],
      "source": [
        "print(chain({\"cuisine\": \"indian\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4069a75e",
      "metadata": {
        "id": "4069a75e"
      },
      "source": [
        "##**06. Agents and Tools**\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "\n",
        "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
        "\n",
        "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "- LLM: The language model powering the agent.\n",
        "- Agent: The agent to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z-4QjS31LD_s",
      "metadata": {
        "id": "Z-4QjS31LD_s"
      },
      "source": [
        "Agent is a very powerful concept in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgNLQ6kSL4na",
      "metadata": {
        "id": "GgNLQ6kSL4na"
      },
      "source": [
        "For example I have to travel from Dubai to Canada, I type this in ChatGPT\n",
        "\n",
        "\n",
        "\n",
        "---> Give me  two flight options from Dubai to Canada on September 1, 2024 | ChatGPT will not be able to answer because has knowledge till\n",
        "September 2021\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tkzApnDnJy8p",
      "metadata": {
        "id": "tkzApnDnJy8p"
      },
      "source": [
        "SerpApi is a real-time API to access Google search results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cd3a12",
      "metadata": {
        "id": "09cd3a12"
      },
      "source": [
        "#### Wikipedia and llm-math tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpJ3gA4YZKMx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpJ3gA4YZKMx",
        "outputId": "74d92f6c-8f46-4d5a-b01d-4aafc01df22c"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kb7ZpkpcVIYO",
      "metadata": {
        "id": "kb7ZpkpcVIYO"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvJNqWvTVKcd",
      "metadata": {
        "id": "UvJNqWvTVKcd"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d06ce6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "14d06ce6",
        "outputId": "eee796fb-5a14-4252-8e8a-7be7207415c4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# install this package: pip install wikipedia\n",
        "\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Let's test it out!\n",
        "\n",
        "\n",
        "agent.run(\"What was the GDP of US in 2024?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6be7ee7",
      "metadata": {
        "id": "b6be7ee7"
      },
      "source": [
        "##**07: Memory**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-WkJqQzRZaXL",
      "metadata": {
        "id": "-WkJqQzRZaXL"
      },
      "source": [
        "Chatbot application like ChatGPT, you will notice that it remember past information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iqunzha0Ztuz",
      "metadata": {
        "id": "Iqunzha0Ztuz"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NE-poGM1Zxss",
      "metadata": {
        "id": "NE-poGM1Zxss"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acab5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2acab5d0",
        "outputId": "49615fa1-90f1-4eaa-ccd1-a46842a776df"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc200f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bc200f9",
        "outputId": "d077dc9a-733f-464a-aad8-62b7e5ef7cf9"
      },
      "outputs": [],
      "source": [
        "name = chain.run(\"Indian\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229a6888",
      "metadata": {
        "id": "229a6888"
      },
      "outputs": [],
      "source": [
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f492fb5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f492fb5a",
        "outputId": "f54b8c42-21ae-4b7a-d748-99a8e0db5c1e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871492be",
      "metadata": {
        "id": "871492be"
      },
      "source": [
        "##**ConversationBufferMemory**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coQKpk8jZ8zz",
      "metadata": {
        "id": "coQKpk8jZ8zz"
      },
      "source": [
        "We can attach memory to remember all previous conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53eea298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53eea298",
        "outputId": "d5c0226a-a886-44be-8fa9-b464a7ed18b9"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de5d50b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de5d50b",
        "outputId": "cb1d1234-5bdf-4bf8-ed4d-77307ff3254e"
      },
      "outputs": [],
      "source": [
        "name = chain.run(\"Arabic\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc88888",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc88888",
        "outputId": "f01949dc-618b-47bc-93a1-a50734773cd7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(chain.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a88b5b",
      "metadata": {
        "id": "a0a88b5b"
      },
      "source": [
        "##**ConversationChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FyFmOOemaVxb",
      "metadata": {
        "id": "FyFmOOemaVxb"
      },
      "source": [
        "Conversation buffer memory goes growing endlessly\n",
        "\n",
        "Just remember last 5 Conversation Chain\n",
        "\n",
        "Just remember last 10-20 Conversation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687ddd2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "687ddd2f",
        "outputId": "57a8866c-7bee-4138-db77-a97d4d9b91ce"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ad5062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "47ad5062",
        "outputId": "460ed904-f145-4a27-8b5e-51c3b567ada5"
      },
      "outputs": [],
      "source": [
        "convo.run(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c80b54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03c80b54",
        "outputId": "b3d62890-1d0e-481a-cefd-18a2dc2db867"
      },
      "outputs": [],
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07342f88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "07342f88",
        "outputId": "0ab4f1ab-b98b-45b3-ac60-dab12d4fd2fd"
      },
      "outputs": [],
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e459d07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e459d07",
        "outputId": "24a349d0-c018-4a3d-face-3847168134af"
      },
      "outputs": [],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaa3abd",
      "metadata": {
        "id": "feaa3abd"
      },
      "source": [
        "##**ConversationBufferWindowMemory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460eb33c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "460eb33c",
        "outputId": "83ac07a2-254a-4f95-9c15-8799f2aef5d1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d395beaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d395beaf",
        "outputId": "ce645a79-c300-4222-bf33-7a260e56a22b"
      },
      "outputs": [],
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b24745",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "93b24745",
        "outputId": "acf0c9f3-c6f1-4376-92dd-54f4a8927a5e"
      },
      "outputs": [],
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K63Ie5FTvjzo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K63Ie5FTvjzo",
        "outputId": "de320f7f-ff80-49cc-b5cc-0ee89d442a00"
      },
      "outputs": [],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mkFhYXKUmnDO",
      "metadata": {
        "id": "mkFhYXKUmnDO"
      },
      "source": [
        "#**08: Document Loaders**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-9WXxhHCZtTn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9WXxhHCZtTn",
        "outputId": "f54f01e1-b31a-4e4c-8d56-86917e6d1822"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wqlRJc7DmtwA",
      "metadata": {
        "id": "wqlRJc7DmtwA"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/my_paper.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XGTtdS26mt0_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTtdS26mt0_",
        "outputId": "c2a6adbe-7308-4838-f0d6-ecb712bf8802"
      },
      "outputs": [],
      "source": [
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77KUe0qSmt5h",
      "metadata": {
        "id": "77KUe0qSmt5h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bduwbig1mt9c",
      "metadata": {
        "id": "bduwbig1mt9c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FSnv3gV3muBm",
      "metadata": {
        "id": "FSnv3gV3muBm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uO4dx_pNmuFt",
      "metadata": {
        "id": "uO4dx_pNmuFt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
